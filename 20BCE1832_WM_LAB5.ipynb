{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20BCE1832\n",
    "HARINI S\n",
    "WEB MINING LAB\n",
    "WEEK 5 ASSESSMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1:Consider the following collection C\n",
    "\n",
    "Doc 1: 'The sun is the largest celestial body in the solar system',\n",
    "\n",
    "Doc 2: 'The solar system consists of the sun and eight revolving planets',\n",
    "\n",
    "Doc 3: 'Ra was the Egyptian Sun God',\n",
    "\n",
    "Doc 4: 'The Pyramids were the pinnacle of Egyptian architecture',\n",
    "\n",
    "Doc 5: 'The quick brown fox jumps over the lazy dog'\n",
    "\n",
    "Doc 6: \"I want to start learning to charge something in life\"\n",
    "\n",
    "Doc 7: \"reading something about life no one else knows\"\n",
    "\n",
    "Doc 8: \"Never stop learning\"\n",
    "\n",
    "Doc 9: “It’s a lazy day and i have a dog”\n",
    "\n",
    "1. Perform Text-preprocessing on the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Shwetha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Shwetha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Shwetha\\AppData\\Roaming\\nltk_data...\n",
      "Doc 1: sun largest celestial body solar system\n",
      "Doc 2: solar system consists sun eight revolving planet\n",
      "Doc 3: ra egyptian sun god\n",
      "Doc 4: pyramid pinnacle egyptian architecture\n",
      "Doc 5: quick brown fox jump lazy dog\n",
      "Doc 6: want start learning charge something life\n",
      "Doc 7: reading something life one else know\n",
      "Doc 8: never stop learning\n",
      "Doc 9: lazy day dog\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    " \n",
    "\n",
    "# Download NLTK resources if not already downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    " \n",
    "\n",
    "# Sample collection\n",
    "collection = [\n",
    "    'The sun is the largest celestial body in the solar system',\n",
    "    'The solar system consists of the sun and eight revolving planets',\n",
    "    'Ra was the Egyptian Sun God',\n",
    "    'The Pyramids were the pinnacle of Egyptian architecture',\n",
    "    'The quick brown fox jumps over the lazy dog',\n",
    "    'I want to start learning to charge something in life',\n",
    "    'reading something about life no one else knows',\n",
    "    'Never stop learning',\n",
    "    'It’s a lazy day and i have a dog'\n",
    "]\n",
    "\n",
    " \n",
    "\n",
    "# Convert to lowercase, tokenization, and punctuation removal\n",
    "cleaned_collection = []\n",
    "for doc in collection:\n",
    "    doc = doc.lower()\n",
    "    words = word_tokenize(doc)\n",
    "    cleaned_words = [re.sub(r'[^a-zA-Z0-9]', '', word) for word in words if re.sub(r'[^a-zA-Z0-9]', '', word)]\n",
    "    cleaned_collection.append(cleaned_words)\n",
    "\n",
    " \n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_collection = [[word for word in words if word not in stop_words] for words in cleaned_collection]\n",
    "\n",
    " \n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_collection = [[lemmatizer.lemmatize(word) for word in words] for words in filtered_collection]\n",
    "\n",
    " \n",
    "\n",
    "# Join words back into documents\n",
    "preprocessed_collection = [\" \".join(words) for words in lemmatized_collection]\n",
    "\n",
    " \n",
    "\n",
    "for idx, doc in enumerate(preprocessed_collection, start=1):\n",
    "    print(f\"Doc {idx}: {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# As we can see, the stop words such as \"the\" is removed and all other punctuation marks has been removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2:\n",
    "\n",
    "2. Calculate the tf-idf vector for the documents with following steps.\n",
    "\n",
    "· Term Frequency (TF)\n",
    "\n",
    "· Inverse Document Frequency (IDF)\n",
    "\n",
    "· TF * IDF\n",
    "\n",
    "· Vector Space Models and Representation – Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\shwetha\\anaconda\\lib\\site-packages (0.18.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the tf-idf vector for the documents with following steps.\n",
    "\n",
    "· Term Frequency (TF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term Frequency Matrix:\n",
      "[[0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0]\n",
      " [0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 1 1 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0]\n",
      " [1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# List of preprocessed documents\n",
    "preprocessed_documents = [\n",
    "    'sun largest celestial body solar system',\n",
    "    'solar system consists sun eight revolving planets',\n",
    "    'ra egyptian sun god',\n",
    "    'pyramids pinnacle egyptian architecture',\n",
    "    'quick brown fox jumps lazy dog',\n",
    "    'want start learning charge something life',\n",
    "    'reading something life one else knows',\n",
    "    'never stop learning',\n",
    "    'lazy day dog'\n",
    "]\n",
    "\n",
    "# Initialize the CountVectorizer to calculate term frequency\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Transform the preprocessed documents to TF matrix\n",
    "tf_matrix = vectorizer.fit_transform(preprocessed_documents)\n",
    "\n",
    "# Get the terms (words) in the documents\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "# Print the TF matrix\n",
    "print(\"Term Frequency Matrix:\")\n",
    "print(tf_matrix.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The next steps in calculating the TF-IDF vector involve calculating the Inverse Document Frequency (IDF) and then combining the TF and IDF to get the final TF-IDF vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix:\n",
      "[[ 0.          0.44873965  0.          0.44873965  0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.44873965  0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.37901266  0.          0.          0.          0.32954056  0.37901266\n",
      "   0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.40940821\n",
      "   0.          0.          0.          0.40940821  0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.40940821  0.          0.          0.          0.\n",
      "   0.40940821  0.34579269  0.          0.          0.          0.30065677\n",
      "   0.34579269  0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.46831599  0.          0.          0.          0.55447213\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.55447213\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.40718723  0.          0.        ]\n",
      " [ 0.51893807  0.          0.          0.          0.          0.          0.\n",
      "   0.          0.43830336  0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.51893807  0.          0.51893807  0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.42926948  0.          0.          0.          0.\n",
      "   0.36256783  0.          0.          0.          0.42926948  0.\n",
      "   0.42926948  0.          0.          0.36256783  0.          0.          0.\n",
      "   0.          0.          0.          0.          0.42926948  0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.          0.          0.44107559  0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.37253947  0.37253947  0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.37253947  0.44107559  0.          0.          0.\n",
      "   0.44107559]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.42926948  0.          0.          0.\n",
      "   0.42926948  0.          0.          0.          0.36256783  0.\n",
      "   0.42926948  0.          0.          0.          0.          0.\n",
      "   0.42926948  0.          0.          0.36256783  0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.51274835  0.          0.60707871\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.60707871  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.64192944  0.54218382  0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.54218382  0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.        ]]\n",
      "Terms with IDF values:\n",
      "architecture: 2.6094\n",
      "body: 2.6094\n",
      "brown: 2.6094\n",
      "celestial: 2.6094\n",
      "charge: 2.6094\n",
      "consists: 2.6094\n",
      "day: 2.6094\n",
      "dog: 2.2040\n",
      "egyptian: 2.2040\n",
      "eight: 2.6094\n",
      "else: 2.6094\n",
      "fox: 2.6094\n",
      "god: 2.6094\n",
      "jumps: 2.6094\n",
      "knows: 2.6094\n",
      "largest: 2.6094\n",
      "lazy: 2.2040\n",
      "learning: 2.2040\n",
      "life: 2.2040\n",
      "never: 2.6094\n",
      "one: 2.6094\n",
      "pinnacle: 2.6094\n",
      "planets: 2.6094\n",
      "pyramids: 2.6094\n",
      "quick: 2.6094\n",
      "ra: 2.6094\n",
      "reading: 2.6094\n",
      "revolving: 2.6094\n",
      "solar: 2.2040\n",
      "something: 2.2040\n",
      "start: 2.6094\n",
      "stop: 2.6094\n",
      "sun: 1.9163\n",
      "system: 2.2040\n",
      "want: 2.6094\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# Initialize the TfidfTransformer to calculate TF-IDF\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "# Fit the TF matrix and calculate IDF\n",
    "tfidf_matrix = tfidf_transformer.fit_transform(tf_matrix)\n",
    "\n",
    "# Print the TF-IDF matrix\n",
    "print(\"TF-IDF Matrix:\")\n",
    "print(tfidf_matrix.toarray())\n",
    "\n",
    "# Get the IDF values\n",
    "idf_values = tfidf_transformer.idf_\n",
    "\n",
    "# Combine terms with their corresponding IDF values\n",
    "term_idf = dict(zip(terms, idf_values))\n",
    "\n",
    "# Print terms with their IDF values\n",
    "print(\"Terms with IDF values:\")\n",
    "for term, idf in term_idf.items():\n",
    "    print(f\"{term}: {idf:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hence the tf-idf vector is calculated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector Space Models and Representation – Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity Matrix:\n",
      "[[ 1.          0.36119821  0.13418471  0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.36119821  1.          0.1224236   0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.13418471  0.1224236   1.          0.20526447  0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.          0.          0.20526447  1.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.          0.          0.          0.          1.          0.          0.\n",
      "   0.          0.39315683]\n",
      " [ 0.          0.          0.          0.          0.          1.\n",
      "   0.27014166  0.191019    0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.27014166\n",
      "   1.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.191019    0.\n",
      "   1.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.39315683  0.          0.\n",
      "   0.          1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate cosine similarity between documents\n",
    "cosine_similarities = cosine_similarity(tfidf_matrix)\n",
    "# Print the cosine similarity matrix\n",
    "print(\"Cosine Similarity Matrix:\")\n",
    "print(cosine_similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The cosine similarity is also printed for reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3:\n",
    "\n",
    "3. Compute the score of each document in C relative to the following queries using the cosine similarity measure.\n",
    "\n",
    "Query 1: “life learning”\n",
    "\n",
    "Query 2: “solar sun”\n",
    "\n",
    "Query 3: “Egyptian”\n",
    "\n",
    "Query 4: “fox lazy”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity scores for Query: {'life learning'}\n",
      "  Document: {1} {0.0}\n",
      "  Document: {2} {0.0}\n",
      "  Document: {3} {0.0}\n",
      "  Document: {4} {0.0}\n",
      "  Document: {5} {0.0}\n",
      "  Document: {6} {0.52685036765432147}\n",
      "  Document: {7} {0.25637417377685223}\n",
      "  Document: {8} {0.36256783359742117}\n",
      "  Document: {9} {0.0}\n",
      "\n",
      "Cosine similarity scores for Query: {'solar sun'}\n",
      "  Document: {1} {0.50224254707426041}\n",
      "  Document: {2} {0.45822164665658438}\n",
      "  Document: {3} {0.26717113355824096}\n",
      "  Document: {4} {0.0}\n",
      "  Document: {5} {0.0}\n",
      "  Document: {6} {0.0}\n",
      "  Document: {7} {0.0}\n",
      "  Document: {8} {0.0}\n",
      "  Document: {9} {0.0}\n",
      "\n",
      "Cosine similarity scores for Query: {'Egyptian'}\n",
      "  Document: {1} {0.0}\n",
      "  Document: {2} {0.0}\n",
      "  Document: {3} {0.46831598990149642}\n",
      "  Document: {4} {0.43830335717994501}\n",
      "  Document: {5} {0.0}\n",
      "  Document: {6} {0.0}\n",
      "  Document: {7} {0.0}\n",
      "  Document: {8} {0.0}\n",
      "  Document: {9} {0.0}\n",
      "\n",
      "Cosine similarity scores for Query: {'fox lazy'}\n",
      "  Document: {1} {0.0}\n",
      "  Document: {2} {0.0}\n",
      "  Document: {3} {0.0}\n",
      "  Document: {4} {0.0}\n",
      "  Document: {5} {0.56189653583178778}\n",
      "  Document: {6} {0.0}\n",
      "  Document: {7} {0.0}\n",
      "  Document: {8} {0.0}\n",
      "  Document: {9} {0.34984806219843728}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Preprocessed documents\n",
    "preprocessed_documents = [\n",
    "    \"sun largest celestial body solar system\",\n",
    "    \"solar system consists sun eight revolving planets\",\n",
    "    \"ra egyptian sun god\",\n",
    "    \"pyramids pinnacle egyptian architecture\",\n",
    "    \"quick brown fox jumps lazy dog\",\n",
    "    \"want start learning charge something life\",\n",
    "    \"reading something life one else knows\",\n",
    "    \"never stop learning\",\n",
    "    \"lazy day dog\"\n",
    "]\n",
    "\n",
    "# Queries\n",
    "queries = [\n",
    "    \"life learning\",\n",
    "    \"solar sun\",\n",
    "    \"Egyptian\",\n",
    "    \"fox lazy\"\n",
    "]\n",
    "\n",
    "# Initialize the TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Calculate TF-IDF vectors for documents\n",
    "tfidf_vectors = vectorizer.fit_transform(preprocessed_documents)\n",
    "\n",
    "# Calculate TF-IDF vectors for queries\n",
    "query_vectors = vectorizer.transform(queries)\n",
    "\n",
    "# Calculate cosine similarity between query vectors and document vectors\n",
    "cosine_similarities = cosine_similarity(query_vectors, tfidf_vectors)\n",
    "\n",
    "# Print the cosine similarity scores\n",
    "for i,query in enumerate(queries):\n",
    "    print(\"Cosine similarity scores for Query:\",{query})\n",
    "    for j,score in enumerate(cosine_similarities[i]):\n",
    "        print(\"  Document:\",{j+1},{score})\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4:\n",
    "\n",
    "4. Recommend the top similar documents, according to the similarity values, rank them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top similar documents for Query: {'life learning'}\n",
      "  Rank: {1}\n",
      "Document {6}\n",
      "Similarity Score: {0.52685036765432147}\n",
      "  Rank: {2}\n",
      "Document {8}\n",
      "Similarity Score: {0.36256783359742117}\n",
      "  Rank: {3}\n",
      "Document {7}\n",
      "Similarity Score: {0.25637417377685223}\n",
      "  Rank: {4}\n",
      "Document {1}\n",
      "Similarity Score: {0.0}\n",
      "  Rank: {5}\n",
      "Document {2}\n",
      "Similarity Score: {0.0}\n",
      "  Rank: {6}\n",
      "Document {3}\n",
      "Similarity Score: {0.0}\n",
      "  Rank: {7}\n",
      "Document {4}\n",
      "Similarity Score: {0.0}\n",
      "  Rank: {8}\n",
      "Document {5}\n",
      "Similarity Score: {0.0}\n",
      "  Rank: {9}\n",
      "Document {9}\n",
      "Similarity Score: {0.0}\n",
      "\n",
      "Top similar documents for Query: {'solar sun'}\n",
      "  Rank: {1}\n",
      "Document {1}\n",
      "Similarity Score: {0.50224254707426041}\n",
      "  Rank: {2}\n",
      "Document {2}\n",
      "Similarity Score: {0.45822164665658438}\n",
      "  Rank: {3}\n",
      "Document {3}\n",
      "Similarity Score: {0.26717113355824096}\n",
      "  Rank: {4}\n",
      "Document {4}\n",
      "Similarity Score: {0.0}\n",
      "  Rank: {5}\n",
      "Document {5}\n",
      "Similarity Score: {0.0}\n",
      "  Rank: {6}\n",
      "Document {6}\n",
      "Similarity Score: {0.0}\n",
      "  Rank: {7}\n",
      "Document {7}\n",
      "Similarity Score: {0.0}\n",
      "  Rank: {8}\n",
      "Document {8}\n",
      "Similarity Score: {0.0}\n",
      "  Rank: {9}\n",
      "Document {9}\n",
      "Similarity Score: {0.0}\n",
      "\n",
      "Top similar documents for Query: {'Egyptian'}\n",
      "  Rank: {1}\n",
      "Document {3}\n",
      "Similarity Score: {0.46831598990149642}\n",
      "  Rank: {2}\n",
      "Document {4}\n",
      "Similarity Score: {0.43830335717994501}\n",
      "  Rank: {3}\n",
      "Document {1}\n",
      "Similarity Score: {0.0}\n",
      "  Rank: {4}\n",
      "Document {2}\n",
      "Similarity Score: {0.0}\n",
      "  Rank: {5}\n",
      "Document {5}\n",
      "Similarity Score: {0.0}\n",
      "  Rank: {6}\n",
      "Document {6}\n",
      "Similarity Score: {0.0}\n",
      "  Rank: {7}\n",
      "Document {7}\n",
      "Similarity Score: {0.0}\n",
      "  Rank: {8}\n",
      "Document {8}\n",
      "Similarity Score: {0.0}\n",
      "  Rank: {9}\n",
      "Document {9}\n",
      "Similarity Score: {0.0}\n",
      "\n",
      "Top similar documents for Query: {'fox lazy'}\n",
      "  Rank: {1}\n",
      "Document {5}\n",
      "Similarity Score: {0.56189653583178778}\n",
      "  Rank: {2}\n",
      "Document {9}\n",
      "Similarity Score: {0.34984806219843728}\n",
      "  Rank: {3}\n",
      "Document {1}\n",
      "Similarity Score: {0.0}\n",
      "  Rank: {4}\n",
      "Document {2}\n",
      "Similarity Score: {0.0}\n",
      "  Rank: {5}\n",
      "Document {3}\n",
      "Similarity Score: {0.0}\n",
      "  Rank: {6}\n",
      "Document {4}\n",
      "Similarity Score: {0.0}\n",
      "  Rank: {7}\n",
      "Document {6}\n",
      "Similarity Score: {0.0}\n",
      "  Rank: {8}\n",
      "Document {7}\n",
      "Similarity Score: {0.0}\n",
      "  Rank: {9}\n",
      "Document {8}\n",
      "Similarity Score: {0.0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Preprocessed documents\n",
    "preprocessed_documents = [\n",
    "    \"sun largest celestial body solar system\",\n",
    "    \"solar system consists sun eight revolving planets\",\n",
    "    \"ra egyptian sun god\",\n",
    "    \"pyramids pinnacle egyptian architecture\",\n",
    "    \"quick brown fox jumps lazy dog\",\n",
    "    \"want start learning charge something life\",\n",
    "    \"reading something life one else knows\",\n",
    "    \"never stop learning\",\n",
    "    \"lazy day dog\"\n",
    "]\n",
    "\n",
    "# Queries\n",
    "queries = [\n",
    "    \"life learning\",\n",
    "    \"solar sun\",\n",
    "    \"Egyptian\",\n",
    "    \"fox lazy\"\n",
    "]\n",
    "\n",
    "# Initialize the TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Calculate TF-IDF vectors for documents\n",
    "tfidf_vectors = vectorizer.fit_transform(preprocessed_documents)\n",
    "\n",
    "# Calculate TF-IDF vectors for queries\n",
    "query_vectors = vectorizer.transform(queries)\n",
    "\n",
    "# Calculate cosine similarity between query vectors and document vectors\n",
    "cosine_similarities = cosine_similarity(query_vectors, tfidf_vectors)\n",
    "\n",
    "# Recommend top similar documents for each query\n",
    "for i, query in enumerate(queries):\n",
    "    print(\"Top similar documents for Query:\",{query})\n",
    "    similarity_scores = list(cosine_similarities[i])\n",
    "    sorted_indices = sorted(range(len(similarity_scores)), key=lambda k: similarity_scores[k], reverse=True)\n",
    "    for rank, doc_index in enumerate(sorted_indices, start=1):\n",
    "        print(\"  Rank:\",{rank})\n",
    "        print(\"Document\", {doc_index+1})\n",
    "        print(\"Similarity Score:\", {similarity_scores[doc_index]})\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# this is sorted in descending order of their cosine similarity scores for each query and has printed the top similar documents along with their corresponding similarity scores and ranks. higher score indicates greater similarity to the query."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
