{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**HARINI S\n",
        "20BCE1832**\n",
        "WEB MINING LAB 7"
      ],
      "metadata": {
        "id": "aaLBzqf9B5gw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1:Write a Naïve Bayes Classifier in python without using any direct ML package for the below Question"
      ],
      "metadata": {
        "id": "uBVSMI7dB5LN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "DBFW1nnjsoCK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b7a1f88-9cec-402a-f43a-2dac6968e768"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total documents: 7\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset from the CSV file\n",
        "df = pd.read_csv(\"data.csv\")\n",
        "\n",
        "# Separate the dataset into features (X) and labels (Y)\n",
        "X = df.iloc[:, :-1]  # Features\n",
        "Y = df.iloc[:, -1]   # Labels\n",
        "\n",
        "# Calculate the total number of documents\n",
        "total_documents = len(df)\n",
        "print (\"Total documents:\",total_documents)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the prior probabilities for each class/category\n",
        "class_counts = Y.value_counts()\n",
        "prior_probabilities = class_counts / total_documents\n",
        "\n",
        "# Laplace smoothing parameter (alpha)\n",
        "alpha = 1\n",
        "print(\"Prior probabilities:\",prior_probabilities)"
      ],
      "metadata": {
        "id": "YF73-dRsszAF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97dc1ea8-7ab9-47d5-dbe3-0562d9274a34"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prior probabilities: Business    0.428571\n",
            "Sports      0.285714\n",
            "Politics    0.285714\n",
            "Name: Category, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a dictionary to store conditional probabilities\n",
        "conditional_probs = {}\n",
        "\n",
        "# Calculate conditional probabilities for each feature and class\n",
        "for category in class_counts.index:\n",
        "    category_data = df[df['Category'] == category]\n",
        "    total_words_in_category = category_data.iloc[:, :-1].values.sum() + alpha * X.shape[1]\n",
        "\n",
        "    conditional_probs[category] = {}\n",
        "    for feature in X.columns:\n",
        "        word_count_in_category = category_data[feature].sum() + alpha\n",
        "        prob_word_given_category = word_count_in_category / total_words_in_category\n",
        "        conditional_probs[category][feature] = prob_word_given_category"
      ],
      "metadata": {
        "id": "HEAU0UNRs2T1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to classify a query using Naïve Bayes\n",
        "def classify_query(query_data):\n",
        "    class_scores = {}\n",
        "    for category in class_counts.index:\n",
        "        score = np.log(prior_probabilities[category])  # Log prior probability\n",
        "        for feature, word_count in zip(X.columns, query_data):\n",
        "            prob_word_given_category = conditional_probs[category].get(feature, 0)\n",
        "            if prob_word_given_category == 0:\n",
        "                prob_word_given_category = alpha / (total_words_in_category + alpha * X.shape[1])\n",
        "            score += word_count * np.log(prob_word_given_category)\n",
        "        class_scores[category] = score\n",
        "    return max(class_scores, key=class_scores.get)"
      ],
      "metadata": {
        "id": "BoazlODrs6Bt"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Queries to classify\n",
        "query_data_1 = [4, 0, 2, 0, 1, 0, 6, 0]\n",
        "query_data_2 = [0, 0, 2, 0, 0, 9, 0, 9]\n",
        "query_data_3 = [5, 0, 2, 5, 0, 9, 0, 9]\n",
        "\n",
        "# Classify the queries\n",
        "result_1 = classify_query(query_data_1)\n",
        "result_2 = classify_query(query_data_2)\n",
        "result_3 = classify_query(query_data_3)\n",
        "\n",
        "# Print the results\n",
        "print(\"Query 1:\", result_1)\n",
        "print(\"Query 2:\", result_2)\n",
        "print(\"Query 3:\", result_3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BaugW8_ustjB",
        "outputId": "cc5d2216-2655-411f-c2ef-914d9b7042ee"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query 1: Politics\n",
            "Query 2: Sports\n",
            "Query 3: Sports\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the total number of documents for each class\n",
        "total_politics_docs = len(df)\n",
        "total_business_docs = len(df)\n",
        "total_sports_docs = len(df)\n",
        "\n",
        "#print results\n",
        "print(\"Total Politics documents:\",total_politics_docs)\n",
        "print(\"Total Business documents:\",total_business_docs)\n",
        "print(\"Total Sports documents:\",total_sports_docs)"
      ],
      "metadata": {
        "id": "EDa3pYQms7Qk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4603f381-264b-4fb8-845f-3dc888f9d229"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Politics documents: 7\n",
            "Total Business documents: 7\n",
            "Total Sports documents: 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the prior probabilities\n",
        "prior_politics = total_politics_docs / (total_politics_docs + total_business_docs + total_sports_docs)\n",
        "prior_business = total_business_docs / (total_politics_docs + total_business_docs + total_sports_docs)\n",
        "prior_sports = total_sports_docs / (total_politics_docs + total_business_docs + total_sports_docs)\n",
        "\n",
        "#print results\n",
        "print(\"politics:\",prior_politics)\n",
        "print(\"business:\",prior_business)\n",
        "print(\"sports:\",prior_sports)"
      ],
      "metadata": {
        "id": "8fvdWDI1tIE7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a42cb57b-ca7e-4575-80a7-7e161c7ad896"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "politics: 0.3333333333333333\n",
            "business: 0.3333333333333333\n",
            "sports: 0.3333333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Query data for each query\n",
        "query_data_1 = [4, 0, 2, 0, 1, 0, 6, 0]\n",
        "query_data_2 = [0, 0, 2, 0, 0, 9, 0, 9]\n",
        "query_data_3 = [5, 0, 2, 5, 0, 9, 0, 9]\n",
        "\n",
        "# Calculate Laplace-smoothed conditional probabilities for each query and class\n",
        "alpha = 1  # Laplace smoothing parameter\n",
        "\n",
        "def calculate_conditional_probability(query_data, class_name):\n",
        "    conditional_prob = 1\n",
        "    total_words_in_class = df[class_name].sum()\n",
        "    for i, word_count in enumerate(query_data):\n",
        "        prob_word_given_class = (df.iloc[i][class_name] + alpha) / (total_words_in_class + alpha * len(df))\n",
        "        conditional_prob *= prob_word_given_class ** word_count\n",
        "    return conditional_prob"
      ],
      "metadata": {
        "id": "Qpd4TnJ_tLtU"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate conditional probabilities for each query and class\n",
        "prob_politics_given_query_1 = prior_politics * calculate_conditional_probability(query_data_1, 'Politics')\n",
        "prob_business_given_query_1 = prior_business * calculate_conditional_probability(query_data_1, 'Business')\n",
        "prob_sports_given_query_1 = prior_sports * calculate_conditional_probability(query_data_1, 'Sports')\n",
        "\n",
        "prob_politics_given_query_2 = prior_politics * calculate_conditional_probability(query_data_2, 'Politics')\n",
        "prob_business_given_query_2 = prior_business * calculate_conditional_probability(query_data_2, 'Business')\n",
        "prob_sports_given_query_2 = prior_sports * calculate_conditional_probability(query_data_2, 'Sports')\n",
        "\n",
        "prob_politics_given_query_3 = prior_politics * calculate_conditional_probability(query_data_3, 'Politics')\n",
        "prob_business_given_query_3 = prior_business * calculate_conditional_probability(query_data_3, 'Business')\n",
        "prob_sports_given_query_3 = prior_sports * calculate_conditional_probability(query_data_3, 'Sports')"
      ],
      "metadata": {
        "id": "ebCw2b8DtWjD"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify the class with the maximum probability for each query\n",
        "max_prob_1 = max(prob_politics_given_query_1, prob_business_given_query_1, prob_sports_given_query_1)\n",
        "max_prob_2 = max(prob_politics_given_query_2, prob_business_given_query_2, prob_sports_given_query_2)\n",
        "max_prob_3 = max(prob_politics_given_query_3, prob_business_given_query_3, prob_sports_given_query_3)\n",
        "\n",
        "if max_prob_1 == prob_politics_given_query_1:\n",
        "    result_1 = \"Politics\"\n",
        "elif max_prob_1 == prob_business_given_query_1:\n",
        "    result_1 = \"Business\"\n",
        "else:\n",
        "    result_1 = \"Sports\"\n",
        "\n",
        "if max_prob_2 == prob_politics_given_query_2:\n",
        "    result_2 = \"Politics\"\n",
        "elif max_prob_2 == prob_business_given_query_2:\n",
        "    result_2 = \"Business\"\n",
        "else:\n",
        "    result_2 = \"Sports\"\n",
        "\n",
        "if max_prob_3 == prob_politics_given_query_3:\n",
        "    result_3 = \"Politics\"\n",
        "elif max_prob_3 == prob_business_given_query_3:\n",
        "    result_3 = \"Business\"\n",
        "else:\n",
        "    result_3 = \"Sports\"\n",
        "\n",
        "print(\"Query 1:\", result_1)\n",
        "print(\"Query 2:\", result_2)\n",
        "print(\"Query 3:\", result_3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZxo4hnHtZvw",
        "outputId": "2b14fd4f-44d7-4659-ab3e-f57851c154f1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query 1: Politics\n",
            "Query 2: Business\n",
            "Query 3: Sports\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2:Take a set of 10 web pages, perform web scraping to extract its contents, do the necessary preprocessing and build a Naïve Bayes Classifier and draw the inferences. You may use any predefined package necessary to perform the above task."
      ],
      "metadata": {
        "id": "D2jEQkgXCTqW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Install required libraries:*"
      ],
      "metadata": {
        "id": "luwbiYCECh_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install requests beautifulsoup4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tlEc9gvCYlG",
        "outputId": "a5cbf687-22d9-4044-ad08-17be4497dcf6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.11.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2023.7.22)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Scrape Data from Web Pages:*"
      ],
      "metadata": {
        "id": "XiGDmQ1fClaB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# List of URLs to scrape\n",
        "urls = [\n",
        "    \"https://www.bbc.com/news\",\n",
        "    \"https://www.nytimes.com/\",\n",
        "    \"https://edition.cnn.com/\",\n",
        "    \"https://www.healthline.com/\",\n",
        "    \"https://www.nba.com/\",\n",
        "    \"https://www.espncricinfo.com/\",\n",
        "    \"https://techcrunch.com/\",\n",
        "    \"https://arstechnica.com/\",\n",
        "    \"https://www.theverge.com/\",\n",
        "    \"https://www.mayoclinic.org/\"\n",
        "]\n",
        "\n",
        "# Initialize an empty list to store the scraped text from each page\n",
        "scraped_data = []\n",
        "\n",
        "for url in urls:\n",
        "    try:\n",
        "        # Send an HTTP GET request to the URL\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Raise an exception if the request fails\n",
        "\n",
        "        # Parse the HTML content of the page\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Extract and store the text content\n",
        "        page_text = soup.get_text()\n",
        "        scraped_data.append(page_text)\n",
        "\n",
        "        print(f\"Scraped data from {url}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping data from {url}: {e}\")\n",
        "\n",
        "# You now have a list of text content from the web pages in the `scraped_data` list."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PjwL7nwClCc",
        "outputId": "73ac5e84-c8d2-4a25-9283-c0ce2d70458f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraped data from https://www.bbc.com/news\n",
            "Scraped data from https://www.nytimes.com/\n",
            "Scraped data from https://edition.cnn.com/\n",
            "Scraped data from https://www.healthline.com/\n",
            "Scraped data from https://www.nba.com/\n",
            "Scraped data from https://www.espncricinfo.com/\n",
            "Scraped data from https://techcrunch.com/\n",
            "Scraped data from https://arstechnica.com/\n",
            "Scraped data from https://www.theverge.com/\n",
            "Scraped data from https://www.mayoclinic.org/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Data Preprocessing:*"
      ],
      "metadata": {
        "id": "yby5MbcWCppY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Sample text for preprocessing\n",
        "text = \"This is a sample text for preprocessing using NLTK.\"\n",
        "\n",
        "# Tokenization\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Removing stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "# Print the filtered tokens\n",
        "print(filtered_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mfq7JUoPEUFD",
        "outputId": "2ae400b5-cc4a-40d5-dfe0-9e46b839610e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sample', 'text', 'preprocessing', 'using', 'NLTK', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*TF-IDF Matrix and Naïve Bayes Classifier:*"
      ],
      "metadata": {
        "id": "EnBuEPAXCsj5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Sample text data\n",
        "corpus = [\"This is the first document.\",\n",
        "          \"This document is the second document.\",\n",
        "          \"And this is the third one.\",\n",
        "          \"Is this the first document?\"]\n",
        "\n",
        "# Create a TF-IDF vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the corpus\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Print the TF-IDF matrix\n",
        "print(tfidf_matrix.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLzBB0dFEhV9",
        "outputId": "dba18e45-2301-4561-90d0-37730a017304"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]\n",
            " [0.         0.6876236  0.         0.28108867 0.         0.53864762\n",
            "  0.28108867 0.         0.28108867]\n",
            " [0.51184851 0.         0.         0.26710379 0.51184851 0.\n",
            "  0.26710379 0.51184851 0.26710379]\n",
            " [0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Train the Naïve Bayes Model*"
      ],
      "metadata": {
        "id": "inXUNIyoEpki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Sample data (replace with your data)\n",
        "X = tfidf_matrix.toarray()  # TF-IDF matrix\n",
        "y = [0, 1, 1, 0]  # Labels (0 for one category, 1 for another)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Multinomial Naïve Bayes classifier\n",
        "clf = MultinomialNB()\n",
        "\n",
        "# Train the classifier\n",
        "clf.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "xEZWe9vjGOBD",
        "outputId": "6b1b56e2-4c93-45b2-bf2a-8273d27199e4"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB()"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Classify New Data:*"
      ],
      "metadata": {
        "id": "n_5LUvDBCwUp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess and TF-IDF transform the new data (replace with your new data)\n",
        "new_data = [\"A political debate on current issues.\", \"Earnings report from a major corporation.\", \"Basketball game results.\"]\n",
        "\n",
        "# Preprocess the new data (tokenization and stop word removal)\n",
        "new_tokens = [word for word in word_tokenize(new_data[0]) if word.lower() not in stop_words]\n",
        "\n",
        "# Convert the new data into TF-IDF representation\n",
        "new_tfidf = tfidf_vectorizer.transform(new_tokens)\n",
        "\n",
        "# Predict the category of the new data\n",
        "prediction = clf.predict(new_tfidf)\n",
        "\n",
        "# Print the prediction\n",
        "print(\"Predicted Category:\", prediction[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyPDgHphElGf",
        "outputId": "335ffd73-3998-434d-c142-ce3fa1a149de"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Category: 0\n"
          ]
        }
      ]
    }
  ]
}